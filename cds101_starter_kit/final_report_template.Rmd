---
title: "CDS 101 – Final Project Report"
author: "Ankith Kolapalli, Julia Conway, Ruhama"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    css: "gmu-cds101.css"
    toc: true
    toc_depth: 3
    number_sections: true
  pdf_document:
    toc: true
---

> Replace this template text with your own writing, keeping the headings and overall structure aligned with the **CDS 101 project rubric**.

# 1. Problem Definition

- Clearly state the main problem or question you are investigating.  
- Explain why it is interesting or important.  
- Define the objective(s) of your analysis.  
- Mention any key assumptions.

# 2. Data Acquisition & Description

Data Set Name: CarDekho Vehicle Dataset
Source: Kaggle (https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho)
Variable description: the dataset has car features like brand, body type, engine type, distance, registration, and price.  
Size: around 4,300 rows with 9 coloumns  


```{r}
library(readr)
library(dplyr)
library(ggplot2)
cars_raw <- read_csv("data/cars_data.csv")
```

# 3. Data Cleaning & Preprocessing

Describe the steps you took to make the data usable, for example:

- Handling missing values (drop, impute, etc.).  
- Fixing types (dates, numeric vs. categorical).  
- Removing outliers (if done).  
- Creating new features (feature engineering).  
- Any scaling or encoding.

```{r cleaning, eval=FALSE}
# Example pseudo-code for cleaning:
# data_clean <- data |>
#   dplyr::filter(!is.na(target_variable)) |>
#   dplyr::mutate(
#     new_feature = some_transformation(old_feature)
#   )
```

```{r}
cars_clean <- cars_raw %>%
  filter(!is.na(Price), !is.na(Brand)) %>% 
  rename(EngineType = `Engine Type`) %>% 
  mutate(Brand = factor(Brand),
         Body = factor(Body),
         EngineType = factor(EngineType),
         Registration = factor(Registration)
  )
```

# 4. Exploratory Data Analysis (EDA)

This section maps directly to the **EDA** part of the rubric:

- Summary statistics  
- Visualizations  
- Interpretation connected to the research question  

```{r}
library(dplyr)
summary(cars_data)
```


```{r}
ggplot(cars_clean) +
  geom_histogram(mapping = aes(x = Price),binwidth = 2000,color = "black",fill  = "blue") +
  coord_cartesian(xlim = c(0, 40000)) +
  labs(title = "Histogram of Used Car Prices",
       x = "Price",
       y = "Count"
  )
```


```{r}
ggplot(cars_clean) +
  geom_histogram(mapping = aes(x = Mileage),binwidth = 18,color = "black",fill  = "green") +
  labs(title = "Histogram of Car Mileage",
       x = "Mileage",
       y = "Count"
  )
```
```{r}
avg_price<- cars_clean %>%
  group_by(Brand) %>%
  summarise(mean_price = mean(Price, na.rm = TRUE)) %>%
  arrange(mean_price)

ggplot(avg_price) +
  geom_col(mapping = aes(x = Brand, y = mean_price, fill = "pink")) +
  labs(title = "Average Used Car Price by Brand",
       x = "Brand",
       y = "Average Price"
  )
```
```{r}
ggplot(cars_clean) +
  geom_point(mapping = aes(x = Mileage, y = Price),
    alpha = 0.4) +
  labs(title = "Relationship Between Mileage and Price",
       x = "Mileage",
       y = "Price"
  )
```
```{r}
ggplot(cars_clean) +
  geom_boxplot(mapping = aes(x = Body, y = Price)) +
  coord_cartesian(ylim = c(0, 200000)) +
  labs(title = "Distribution of Car Prices by Body Type",
       x = "Body Type",
       y = "Price"
  )
```
# 5. Visualization Quality and Storytelling

Use this section to satisfy the **Visualization Quality** rubric criterion:

- Explain why your plot types are appropriate.  
- Comment on labels, legends, colors, and overall readability.  
- Mention any steps you took to make plots accessible and interpretable.

# 6. Modeling Approach


Since our outcome variable we chose was price and its numeric and continuous, a regression model seemed the most appropriate.
We started with a Baseline model and its prupose was to set a reference point for our real model should do better than to be somewhat useful.
The baseline predicted the average price of all cars in the training set for every car in the test. It didn't use any specific features and was a simple guess average.
After our baseline we made or main model which was a multiple linear regression model. It let us look at different features that each had a part in predicting a cars price. Since linear regression is a lot more easier to understand it works best with categorical predictors and values and it was many times throughout the class. By comparing the RMSE and the R^2 of the linear model against the basline we were able to look at how much better our model was perfoming compared to the simple average based prediction.


# 7. Model Implementation & Evaluation

This corresponds to the **Model Implementation & Evaluation** rubric criterion.

Describe:

- Features used.  

Our model used Mileage, Enginev, and Year as numeric predictors and Brand, Bodytype, and Enginetype as categorical. They were chose becuase they're the most common factors that would influence a cars market value.
- Data splitting strategy (train/test or cross-validation).

We split the data into 80% training set and 20% testing. It let us fit the model on the majority of the dat set
and look at how well it can generalize to the cars that are unseen by it.

- Metrics used (accuracy, F1, RMSE, etc.).

The Baseline predicted the average price from the training set for every car in the test set.
It helps us just have a benchmark that we should try to beat with our model.
Our baseline RMSE was 23,146.9 which means the guess is off by around 23k on average.

We built a multiple linear regression model predicting price based on numeric and categorical predictors.
Linear regressions works best becuase our target Price is numeric as well. After training our linear model the RMSE was 16,786.2 which is a lot lower than the baseline.
This meant our model was doing better, and the adjusted R² was 0.4376 which means about 43.8% of the variation in car prices was explained by the  
- A short interpretation of each metric/plot.
The multiple linear regression model clearly did better the baseline model, lowering the error by over $6,000 on average.
The adjusted R² shows our model explains a meaningful part of price variation but doesn't grab  everything.
This makes sense because real-world car pricing depends on so many other factors that aren't part of the data set like car condition, past accidents, location, seller, etc.
The RMSE and R² together say that the model is useful but not perfect which is something consistent with these types of data sets.


```{r}
set.seed(321)
train_index <- sample(seq_len(nrow(cars_clean)), size = 0.8 * nrow(cars_clean))
train <- cars_clean[train_index, ]
test  <- cars_clean[-train_index, ]

baseline <- mean(train$Price, na.rm = TRUE)
baseline_preds <- rep(baseline, nrow(test))
baseline_rmse <- sqrt(mean((test$Price - baseline_preds)^2, na.rm = TRUE))
baseline_rmse

model <- lm(Price ~ Mileage + Brand + Body + EngineV + EngineType + Year,data = train)
preds <- predict(model, newdata = test)
rmse <- sqrt(mean((test$Price - preds)^2, na.rm = TRUE))
rmse



summary(model)$adj.r.squared 
```

# 8. Conclusions & Recommendations

Summarize the key takeaways:

- Answer your original research question(s) directly.  
- Highlight the most important patterns or relationships you found.  
- Discuss limitations (data size, bias, missing variables, etc.).  
- Suggest possible extensions or future work.

# 9. Code Quality & Reproducibility

Briefly document how someone else can reproduce your results:

- Which R scripts or Rmd files to run.  
- Any required R packages.  

```{r session-info, echo=FALSE}
sessionInfo()
```

# 10. References

List any references you used, such as:

- Dataset documentation.  
- Research papers or articles.  
- Tutorials or blog posts.

# Appendix (Optional)

Include extra plots, diagnostic checks, or model comparisons if needed.
